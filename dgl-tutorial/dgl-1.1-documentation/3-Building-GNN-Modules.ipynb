{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Building GNN Modules\n",
    "\n",
    "DGL NN inherits from backends including PyTorch NN, MXNet Gluon NN, and TensorFlow Keras NN\n",
    "\n",
    "DGL has integrated many commonly used apinn-pytorch-conv, apinn-pytorch-dense-conv, apinn-pytorch-pooling, and apinn-pytorch-util.\n",
    "\n",
    "This chapter takes SAGEConv with Pytorch backend as an example to introduce how to build a custom DGL NN Module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DGL NN Module Construction Function\n",
    "\n",
    "The construction function performs three sequential steps:\n",
    "1. Set options\n",
    "2. Register learnable parameters or submodules\n",
    "3. Reset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dgl.utils import expand_as_pair\n",
    "class SAGEConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 aggregator_type, \n",
    "                 bias=True, \n",
    "                 norm=None, \n",
    "                 activation=None):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._aggre_type = aggregator_type\n",
    "        self.norm = norm\n",
    "        self.activation = activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the construction function, we first need to set the data dimensions. For general PyTorch modules, the dimensions are usually input dimension, output dimension and hidden dimensions. For graph neural networks, the input dimension can be split into source node dimension and destination node dimension. \n",
    "\n",
    "Beside data dimensions, a typical option for GNNs is aggregation type (`self._aggre_type`). Aggregation type determines how messages on different edges are aggregated for a certain destination node. Commonly used aggregation types include `mean`, `sum`, `max`, `min`. Some modules may apply more complicated aggregation like `lstm`. \n",
    "\n",
    "`norm` here is a callable function for feature normalization. In the SAGEConv paper normalization can be l2 normalization: $h_v = h_v/||h_v||_2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# aggregator type: mean, pool, lstm, gcn\n",
    "if aggregator_type not in ['mean', 'pool', 'lstm', 'gcn']:\n",
    "    raise KeyError('Aggregator type {} not supported.'.format(aggregator_type))\n",
    "if aggregator_type == 'pool':\n",
    "    self.fc_pool = nn.Linear(self._in_src_feats, self._in_src_feats)\n",
    "if aggregator_type == 'lstm':\n",
    "    self.lstm = nn.LSTM(self._in_src_feats, self._in_src_feats, batch_first=True)\n",
    "if aggregator_type in ['mean', 'pool', 'lstm']:\n",
    "    self.fc_self = nn.Linear(self._in_dst_feats, out_feats, bias=bias)\n",
    "self.fc_neigh = nn.Linear(self._in_src_feats, out_feats, bias=bias)\n",
    "self.reset_parameters()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering parameters and submodules. In SAGEConv, submodules vary according to the aggregation type. Those modules are pure PyTorch nn modules like `nn.Linear`, `nn.LSTM`, etc. At the end of construction function, weight initialization is applied by calling `reset_parameters()`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def reset_parameters(self):\n",
    "    \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "    gain = nn.init.calculate_gain('relu')\n",
    "    if self._aggre_type == 'pool':\n",
    "        nn.init.xavier_uniform_(self.fc_pool.weight, gain=gain)\n",
    "    if self._aggre_type == 'lstm':\n",
    "        self.lstm.reset_parameters()\n",
    "    if self._aggre_type != 'gcn':\n",
    "        nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)\n",
    "    nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DGL NN Module Forward Function\n",
    "\n",
    "In the neural network module, `forward()` function does the actual message passing and computation. \n",
    "\n",
    "Compared with PyTorch's NN module which usually takes tensors as the parameters, DGL's NN module takes an additional parameter `dgl.DGLGraph`. The workload for `forward()` function can be split into three parts:\n",
    "\n",
    "1. Graph checking and graph type specification\n",
    "2. message passing \n",
    "3. Feature Update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside GraphSAGE \n",
    "\n",
    "Source: https://youtu.be/LLUxwHc7O4A\n",
    "\n",
    "GraphSAGE changed the way we think of GNNs through neighbourhood sampling.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def forrward(self, graph, feat):\n",
    "    with graph.local_scope():\n",
    "        # Specify graph type then expand input feature according to graph type\n",
    "        feat_src, feat_dst = expand_as_pair(feat, graph)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward()` needs to handle many corner cases on the input that can lead to invalid values in computing and message passing. One typical check in conv modules like `GraphConv` is to verify that the input graph has no 0-in-degree nodes. When a node has 0 in-degree, the `mailbox` will be empty and the reduce function will produce all-zero values. This may cause silent regression in model performance. However, in the `SAGEConv` module, the aggregated representation will be concatenated with the original node feature, the output of `forward()` will not be all-zero. \n",
    "\n",
    "The DGL NN module should be reusable across different types of graph input including homogeneous graphs, heterogeneous graphs, and subgraph blocks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math formulas for SAGEConv are:\n",
    "\n",
    "$$h_{\\mathcal{N}(dst)}^{(l+1)} = aggregate (\\{h_{src}^l, \\forall src \\in \\mathcal{N}(dst)\\})$$\n",
    "\n",
    "$$ h_{dst}^{(l+1)} = \\sigma (W \\cdot concat(h_{dst}^l, h_{\\mathcal{N}(dst)}^{l+1})+b)$$\n",
    "\n",
    "$$h_{dst}^{(l+1)} = norm(h_{dst}^{(l+1)})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify the source node feature `feat_src` and destination node feature `feat_dst` according to the graph type. `expand_as_pair()` is a function that specifies the graph type and expands `feat` into `feat_src` and `feat_dst`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def expand_as_pair(input, g=None):\n",
    "    if isinstance(input_, tuple):\n",
    "        # Bipartite graph case\n",
    "        return input_\n",
    "    elif g is not None and g.is_block:\n",
    "        # Subgraph block case\n",
    "        if isinstance(input_, Mapping):\n",
    "            input_dst = {\n",
    "                k: F.narrow_row(v, 0, g.number_of_dst_nodes(k)) \n",
    "                for k, v in input_.items()\n",
    "            }\n",
    "        else: \n",
    "            input_dst = F.narrow_row(input_, 0, g.number_of_dst_node())\n",
    "        return input_, input_dst\n",
    "    else:\n",
    "        #Homogeneous graph case\n",
    "        return input_, input_\n",
    "        \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For homogeneous the whole graph training, source nodes and destination nodes are the same. They are all the nodes in the graph. \n",
    "\n",
    "For heterogeneous case, the graph can be split into several bipartite graphs, one for each relation. The relations are represented as `(src_type, edge_type, dst_type)`. When the relation identifies that the input feature `feat` is a tuple, it will treat the graph as bipartite. The first element in the tuple will be the source node feature and the second element will be the destination node feature. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Heterogeneous GraphConv Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
