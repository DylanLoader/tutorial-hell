{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Building GNN Modules\n",
    "\n",
    "DGL NN inherits from backends including PyTorch NN, MXNet Gluon NN, and TensorFlow Keras NN\n",
    "\n",
    "DGL has integrated many commonly used apinn-pytorch-conv, apinn-pytorch-dense-conv, apinn-pytorch-pooling, and apinn-pytorch-util.\n",
    "\n",
    "This chapter takes SAGEConv with Pytorch backend as an example to introduce how to build a custom DGL NN Module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DGL NN Module Construction Function\n",
    "\n",
    "The construction function performs three sequential steps:\n",
    "1. Set options\n",
    "2. Register learnable parameters or submodules\n",
    "3. Reset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dgl.utils import expand_as_pair\n",
    "class SAGEConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 aggregator_type, \n",
    "                 bias=True, \n",
    "                 norm=None, \n",
    "                 activation=None):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._aggre_type = aggregator_type\n",
    "        self.norm = norm\n",
    "        self.activation = activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the construction function, we first need to set the data dimensions. For general PyTorch modules, the dimensions are usually input dimension, output dimension and hidden dimensions. For graph neural networks, the input dimension can be split into source node dimension and destination node dimension. \n",
    "\n",
    "Beside data dimensions, a typical option for GNNs is aggregation type (`self._aggre_type`). Aggregation type determines how messages on different edges are aggregated for a certain destination node. Commonly used aggregation types include `mean`, `sum`, `max`, `min`. Some modules may apply more complicated aggregation like `lstm`. \n",
    "\n",
    "`norm` here is a callable function for feature normalization. In the SAGEConv paper normalization can be l2 normalization: $h_v = h_v/||h_v||_2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# aggregator type: mean, pool, lstm, gcn\n",
    "if aggregator_type not in ['mean', 'pool', 'lstm', 'gcn']:\n",
    "    raise KeyError('Aggregator type {} not supported.'.format(aggregator_type))\n",
    "if aggregator_type == 'pool':\n",
    "    self.fc_pool = nn.Linear(self._in_src_feats, self._in_src_feats)\n",
    "if aggregator_type == 'lstm':\n",
    "    self.lstm = nn.LSTM(self._in_src_feats, self._in_src_feats, batch_first=True)\n",
    "if aggregator_type in ['mean', 'pool', 'lstm']:\n",
    "    self.fc_self = nn.Linear(self._in_dst_feats, out_feats, bias=bias)\n",
    "self.fc_neigh = nn.Linear(self._in_src_feats, out_feats, bias=bias)\n",
    "self.reset_parameters()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering parameters and submodules. In SAGEConv, submodules vary according to the aggregation type. Those modules are pure PyTorch nn modules like `nn.Linear`, `nn.LSTM`, etc. At the end of construction function, weight initialization is applied by calling `reset_parameters()`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def reset_parameters(self):\n",
    "    \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "    gain = nn.init.calculate_gain('relu')\n",
    "    if self._aggre_type == 'pool':\n",
    "        nn.init.xavier_uniform_(self.fc_pool.weight, gain=gain)\n",
    "    if self._aggre_type == 'lstm':\n",
    "        self.lstm.reset_parameters()\n",
    "    if self._aggre_type != 'gcn':\n",
    "        nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)\n",
    "    nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DGL NN Module Forward Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Heterogeneous GraphConv Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
