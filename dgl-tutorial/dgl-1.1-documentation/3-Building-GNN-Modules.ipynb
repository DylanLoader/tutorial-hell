{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Building GNN Modules\n",
    "\n",
    "DGL NN inherits from backends including PyTorch NN, MXNet Gluon NN, and TensorFlow Keras NN\n",
    "\n",
    "DGL has integrated many commonly used apinn-pytorch-conv, apinn-pytorch-dense-conv, apinn-pytorch-pooling, and apinn-pytorch-util.\n",
    "\n",
    "This chapter takes SAGEConv with Pytorch backend as an example to introduce how to build a custom DGL NN Module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DGL NN Module Construction Function\n",
    "\n",
    "The construction function performs three sequential steps:\n",
    "1. Set options\n",
    "2. Register learnable parameters or submodules\n",
    "3. Reset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dgl.utils import expand_as_pair\n",
    "class SAGEConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 aggregator_type, \n",
    "                 bias=True, \n",
    "                 norm=None, \n",
    "                 activation=None):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._aggre_type = aggregator_type\n",
    "        self.norm = norm\n",
    "        self.activation = activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the construction function, we first need to set the data dimensions. For general PyTorch modules, the dimensions are usually input dimension, output dimension and hidden dimensions. For graph neural networks, the input dimension can be split into source node dimension and destination node dimension. \n",
    "\n",
    "Beside data dimensions, a typical option for GNNs is aggregation type (`self._aggre_type`). Aggregation type determines how messages on different edges are aggregated for a certain destination node. Commonly used aggregation types include `mean`, `sum`, `max`, `min`. Some modules may apply more complicated aggregation like `lstm`. \n",
    "\n",
    "`norm` here is a callable function for feature normalization. In the SAGEConv paper normalization can be l2 normalization: $h_v = h_v/||h_v||_2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# aggregator type: mean, pool, lstm, gcn\n",
    "if aggregator_type not in ['mean', 'pool', 'lstm', 'gcn']:\n",
    "    raise KeyError('Aggregator type {} not supported.'.format(aggregator_type))\n",
    "if aggregator_type == 'pool':\n",
    "    self.fc_pool = nn.Linear(self._in_src_feats, self._in_src_feats)\n",
    "if aggregator_type == 'lstm':\n",
    "    self.lstm = nn.LSTM(self._in_src_feats, self._in_src_feats, batch_first=True)\n",
    "if aggregator_type in ['mean', 'pool', 'lstm']:\n",
    "    self.fc_self = nn.Linear(self._in_dst_feats, out_feats, bias=bias)\n",
    "self.fc_neigh = nn.Linear(self._in_src_feats, out_feats, bias=bias)\n",
    "self.reset_parameters()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering parameters and submodules. In SAGEConv, submodules vary according to the aggregation type. Those modules are pure PyTorch nn modules like `nn.Linear`, `nn.LSTM`, etc. At the end of construction function, weight initialization is applied by calling `reset_parameters()`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def reset_parameters(self):\n",
    "    \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "    gain = nn.init.calculate_gain('relu')\n",
    "    if self._aggre_type == 'pool':\n",
    "        nn.init.xavier_uniform_(self.fc_pool.weight, gain=gain)\n",
    "    if self._aggre_type == 'lstm':\n",
    "        self.lstm.reset_parameters()\n",
    "    if self._aggre_type != 'gcn':\n",
    "        nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)\n",
    "    nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DGL NN Module Forward Function\n",
    "\n",
    "In the neural network module, `forward()` function does the actual message passing and computation. \n",
    "\n",
    "Compared with PyTorch's NN module which usually takes tensors as the parameters, DGL's NN module takes an additional parameter `dgl.DGLGraph`. The workload for `forward()` function can be split into three parts:\n",
    "\n",
    "1. Graph checking and graph type specification\n",
    "2. message passing \n",
    "3. Feature Update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside GraphSAGE \n",
    "\n",
    "Source: https://youtu.be/LLUxwHc7O4A\n",
    "\n",
    "GraphSAGE changed the way we think of GNNs through neighbourhood sampling.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def forrward(self, graph, feat):\n",
    "    with graph.local_scope():\n",
    "        # Specify graph type then expand input feature according to graph type\n",
    "        feat_src, feat_dst = expand_as_pair(feat, graph)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward()` needs to handle many corner cases on the input that can lead to invalid values in computing and message passing. One typical check in conv modules like `GraphConv` is to verify that the input graph has no 0-in-degree nodes. When a node has 0 in-degree, the `mailbox` will be empty and the reduce function will produce all-zero values. This may cause silent regression in model performance. However, in the `SAGEConv` module, the aggregated representation will be concatenated with the original node feature, the output of `forward()` will not be all-zero. \n",
    "\n",
    "The DGL NN module should be reusable across different types of graph input including homogeneous graphs, heterogeneous graphs, and subgraph blocks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math formulas for SAGEConv are:\n",
    "\n",
    "$$h_{\\mathcal{N}(dst)}^{(l+1)} = aggregate (\\{h_{src}^l, \\forall src \\in \\mathcal{N}(dst)\\})$$\n",
    "\n",
    "$$ h_{dst}^{(l+1)} = \\sigma (W \\cdot concat(h_{dst}^l, h_{\\mathcal{N}(dst)}^{l+1})+b)$$\n",
    "\n",
    "$$h_{dst}^{(l+1)} = norm(h_{dst}^{(l+1)})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify the source node feature `feat_src` and destination node feature `feat_dst` according to the graph type. `expand_as_pair()` is a function that specifies the graph type and expands `feat` into `feat_src` and `feat_dst`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def expand_as_pair(input, g=None):\n",
    "    if isinstance(input_, tuple):\n",
    "        # Bipartite graph case\n",
    "        return input_\n",
    "    elif g is not None and g.is_block:\n",
    "        # Subgraph block case\n",
    "        if isinstance(input_, Mapping):\n",
    "            input_dst = {\n",
    "                k: F.narrow_row(v, 0, g.number_of_dst_nodes(k)) \n",
    "                for k, v in input_.items()\n",
    "            }\n",
    "        else: \n",
    "            input_dst = F.narrow_row(input_, 0, g.number_of_dst_node())\n",
    "        return input_, input_dst\n",
    "    else:\n",
    "        #Homogeneous graph case\n",
    "        return input_, input_\n",
    "        \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For homogeneous the whole graph training, source nodes and destination nodes are the same. They are all the nodes in the graph. \n",
    "\n",
    "For heterogeneous case, the graph can be split into several bipartite graphs, one for each relation. The relations are represented as `(src_type, edge_type, dst_type)`. When the relation identifies that the input feature `feat` is a tuple, it will treat the graph as bipartite. The first element in the tuple will be the source node feature and the second element will be the destination node feature. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Heterogeneous GraphConv Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HeteroGraphConv` is a module-level encapsulation to run DGL NN module on heterogeneous graphs. The implementation logic is the same as message passing level API `multi_update_all()`, including:\n",
    "\n",
    "- DGL NN module within each relatioin r. \n",
    "- Reduction that merges the results on the same node\n",
    "\n",
    "This can be formulated as:\n",
    "\n",
    "$$h_{dst}^{(1+l)} = \\stackrel{AGG}{{r \\in \\mathcal{R}, r_{dst}=dst}} (f_r(g_r, h_{r_{src}}^l, h_{r_{dst}}^l)$$\n",
    "\n",
    "where $f_r$ is the NN module for each relation r, AGG is the aggregation function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HeteroGraphConv implementation logic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class HeteroGraphConv(nn.Module):\n",
    "    def __init__(self, mods, aggregate='sum'):\n",
    "        super(HeteroGraphConv, self).__init__()\n",
    "        self.mods = nn.ModuleDict(mods)\n",
    "        if isinstance(aggregate, str): \n",
    "            # An internal function to get common aggregation functions. \n",
    "            self.agg_fn = get_aggregate_fn(aggregate)\n",
    "        else:\n",
    "            self.agg_fn = aggregate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heterograph convolution takes a dictionary `mods`that maps each relation to a nn module and sets the function that aggregates results on the same node type from multiple relations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, g, inputs, mod_args=None, mod_kwargs=None):\n",
    "    if mod_args is None:\n",
    "        mod_args = {}\n",
    "    if mod_kwargs is None:\n",
    "        mod_kwargs = {}\n",
    "    outputs = {nty: [] for nty in g.dsttypes}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the input graph and input tensors, `forward()` takes two additional dictionary parameters `mod_args` and `mod_kwargs`. These two dictionaires have the same keys as `self.mods`.They are used as customied parameters when calling their corresponding NN modules in `self.mods` for different types of relations. \n",
    "\n",
    "An output dictioinary is created to hold tensor for each destination type `nty`. The value for each `nty` is a list, indicating a single node type may get multiple outputs if more than one relation have `nty` as the destination type. `HeteroGraphConv` will perform a further aggegation on the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if g.is_block: \n",
    "    src_inputs = inputs \n",
    "    dst_inputs = {k: v[:g.number_of_dst_nodes(k)] for k, v in inputs.items()}\n",
    "else: \n",
    "    src_inputs = dst_inputs = inputs \n",
    "    \n",
    "for stype, etype, dtype in g.canonical_etypes:\n",
    "    rel_graph = g[stype, etype, dtype]\n",
    "    if rel_graph.num_edges()==0:\n",
    "        continue\n",
    "    if stype not in src_inputs or dtype not in dst_inputs:\n",
    "        continue\n",
    "    dstdata = self.mods[etype](\n",
    "        rel_graph, \n",
    "        (src_inputs[stype], dst_inputs[dtype]),\n",
    "        *mod_arg.get(etype, ()),\n",
    "        **mod_kwargs.get(etype, {}))\n",
    "    outputs[dtype].append(dstdata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each relation is represented as a `canonical_etype`, which is `(stype, etype, dtype)`. Using `canonical_etype` as the key, one can extract out a bipartite graph `rel_graph`. For bipartite graph, the input feature will be organized as a tuple `(src_inputs[stype], dst_inputs[dtype])`. The NN module for each relation is called and output is saved. To avoid unnecessary calls, relations with no edges or no nodes with the srctype will be skipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsts = {}\n",
    "for nty, alist in outputs.items():\n",
    "    if len(alist) !=0:\n",
    "        rsts[nty] = self.agg_fn(alist, nty)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the results on the same destination node type from multiple relations are aggregated using `self.agg_fn` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
