{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Training Graph Neural Networks\n",
    "\n",
    "### Overview\n",
    "\n",
    "The following assumes that the graphs(s) and node/edge features are already prepared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl \n",
    "dataset = dgl.data.CiteseerGraphDataset()\n",
    "graph = dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous Graphs\n",
    "\n",
    "Sometimes we would like to work with heterogeneous graphs. Here we take a synthetic heterogeneous graph as an example for demonstrating node classification, edge classification, and link prediction tasks. \n",
    "\n",
    "The synthetic heterogeneous graph `hetero_graph` has these edge types:\n",
    "\n",
    "- `('user', 'follow', 'user')`\n",
    "- `('user', 'followed_by', 'user')`\n",
    "- `('user', 'click', 'item')`\n",
    "- `('item', 'clicked-by', 'user')`\n",
    "- `('user', 'dislike', 'item')`\n",
    "- `('item', 'disliked-by', 'user)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set seeds \n",
    "np.random.seed(1775)\n",
    "torch.manual_seed(1775)\n",
    "\n",
    "n_users = 1000\n",
    "n_items = 500\n",
    "n_follows = 3000\n",
    "n_clicks = 5000\n",
    "n_dislikes = 500\n",
    "n_hetero_features = 10\n",
    "n_user_classes = 5\n",
    "n_max_clicks = 10\n",
    "\n",
    "follow_src = np.random.randint(0, n_users, n_follows)\n",
    "follow_dst = np.random.randint(0, n_users, n_follows)\n",
    "click_src = np.random.randint(0, n_users, n_clicks)\n",
    "click_dst = np.random.randint(0, n_items, n_clicks)\n",
    "dislike_src = np.random.randint(0, n_users, n_dislikes)\n",
    "dislike_dst = np.random.randint(0, n_items, n_dislikes)\n",
    "\n",
    "hetero_graph = dgl.heterograph({\n",
    "    ('user', 'follow', 'user'): (follow_src, follow_dst),\n",
    "    ('user', 'followed-by', 'user'): (follow_dst, follow_src), \n",
    "    ('user', 'click', 'item'): (click_src, click_dst), \n",
    "    ('item', 'clicked-by', 'user'): (click_dst, click_src),\n",
    "    ('user', 'dislike', 'item'): (dislike_src, dislike_dst), \n",
    "    ('item', 'disliked-by', 'user'): (dislike_dst, dislike_src),\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Node Classification/Regression\n",
    "\n",
    "### Writing NN Model\n",
    "\n",
    "DGL provides a few builtin graph convolution modules that can perform one round of message passing. We choose `dgl.nn.pytorch.SAGEConv`, the graph convolution module for GraphSAGE. \n",
    "\n",
    "For deep learning models on graphs we need a multi-layer graph neural network, where we do multiple rounds of message passing. This is achieved by stacking graph convolution modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a two-layer GNN model\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv(\n",
    "            in_feats = in_feats, out_feats=hid_feats, aggregator_type='mean',\n",
    "        ) \n",
    "        self.conv2 = dglnn.SAGEConv(\n",
    "            in_feats = hid_feats, out_feats = out_feats, aggregator_type='mean'\n",
    "        )\n",
    "        \n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Training on the full graph simply involves a forward propagation of the model defined above, and computing the loss by comparing the prediction against ground truth labels on the training nodes. \n",
    "\n",
    "The section uses a DGL built-in dataset `dgl.data.CiteseerGraphDataset` to show a training loop. The node features and labels are stored on its graph instance, and the training-validation-test split are also stored on the graph as boolean masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = graph.ndata['feat']\n",
    "node_labels = graph.ndata['label']\n",
    "train_mask = graph.ndata['train_mask']\n",
    "valid_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "n_features = node_features.shape[1]\n",
    "n_labels = int(node_labels.max().item()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, features)\n",
    "        logtis = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logtis, dim=1)\n",
    "        correct = torch.sum(indices==labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then write the training loop as:\n",
    "\n",
    "model = SAGE(in_feat=n_features, hid_feats =100, out_feats=n_labels)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    # forward propagation by using all nodes\n",
    "    logits = model(graph, node_features)\n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(logits[train_mask], node_labels[train_mask])\n",
    "    # Compute validation accuracy\n",
    "    acc = evaluate(model, graph, node_features, node_labels, valid_mask)\n",
    "    # Back propagation\n",
    "    opt.zero_grad()\n",
    "    loss.backwards()\n",
    "    opt.step()\n",
    "    \n",
    "    #Optionally here we could save the model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSAGE provides an end-to-end homogeneous graph node classification example. You could see the corresponding model implementation is in the `GraphSAGE` class in the the example with adjustable number of layers, dropout probabilities, and customizable aggregation functions and nonlinearities. \n",
    "\n",
    "### Heterogeneous Graph\n",
    "\n",
    "If our graph is heterogeneous, we may want to gather messages from neighbours along all edge types. We can use the module `dgl.nn.pytorch.HeteroGraphConv` to perform message passing on all edge types, then combining different graph convolution modules for each type. \n",
    "\n",
    "The following code will define a heterogeneous graph convolution module that first performs a separate graph convolution on each edge type, then sums the message aggregations on each type as the final result for all node types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a heterograph conv model\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, hid_feats) for rel in rel_names\n",
    "        }, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats) for rel in rel_names\n",
    "        }, aggregate='sum')\n",
    "        \n",
    "    def forward(self, graph, inputs): \n",
    "        # Inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dgl.nn.HeteroGraphConv` takes in a dictionary of node types and node feature tensors as input, and returns another dictionary of node types and node features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the user-item heterograph example\n",
    "model = RGCN(n_hetero_features, 20, n_user_classes, hetero_graph.etypes)\n",
    "user_feats = hetero_graph.nodes['user'].data['feature']\n",
    "item_feats = hetero_graph.nodes['item'].data['feature']\n",
    "labels = hetero_graph.nodes['user'].data['label']\n",
    "train_mask = hetero_graph.nodes['user'].data['train_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "node_features = {'user': user_feats, 'item': item_feats}\n",
    "h_dict = model(hetero_graph, {'users': user_feats, 'item':item_feats})\n",
    "h_user = h_dict['user']\n",
    "h_item = h_dict['item']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop is the same as for the homogeneous graph, except now we have a dictionary of node representations from which you compute the predictions. For instance if we are only predicting the `user` nodes, we can just extract the `user` node embeddings from the returned dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    # forward propagation using all nodes and extracting the user embs\n",
    "    logits = model(hetero_graph, node_features)['user']\n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "    # Compute the validation acc (omitted here)\n",
    "    # back propagation\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Edge Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random graph for edge prediction\n",
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "np.random.seed(1775)\n",
    "torch.manual_seed(1775)\n",
    "\n",
    "src = np.random.randint(0, 100, 500)\n",
    "dst = np.random.randint(0, 100, 500)\n",
    "# Make the graph undirected\n",
    "edge_pred_graph = dgl.graph((np.concatenate([src,dst]), np.concatenate([dst,src])))\n",
    "# synthetic node and edge features, as well as edge labels\n",
    "edge_pred_graph.ndata['feature'] = torch.randn(100,10)\n",
    "edge_pred_graph.edata['feature'] = torch.randn(1000,10)\n",
    "edge_pred_graph.edata['label'] = torch.randn(1000)\n",
    "# synthetic train-validation-test splits\n",
    "edge_pred_graph.edata['train_mask'] = torch.zeros(1000, dtype=torch.bool).bernoulli(0.6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to how node classification is done with a multilayer GNN. The same technique can be applied for computing a hidden representation of any node. The prediction on edges can be derived from the representation of their incident nodes. \n",
    "\n",
    "The most common case of computing the prediction on  an edge is to express it as a parameterized function of the representation of its incident nodes, and optionally the features on the edge itself. \n",
    "\n",
    "### Model Implementation Difference from Node Classification\n",
    "\n",
    "Here we compute the node representation with the model from the previous section on Node Classification, we need only to write another component that computes the edge prediction with `apply_edges()` method. \n",
    "\n",
    "For instance if we want to compute a score for each edge for edge regression, the following code computes the dot product of incident node representations on each edge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "class DotProductPredictor(nn.Module):\n",
    "    def forward(self, graph, h):\n",
    "        # h contains the node representations computed from the GNN defined\n",
    "        # in the node classification section 3.2\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also write a prediction function that predicts a vector for each edge with a MLP. Such a vector can be used in further downstream tasks, for example as logits of a categorical distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features *2, out_classes)\n",
    "        \n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(torch.cat([h_u, h_v], 1))\n",
    "        \n",
    "    def forward(self, graph, h): \n",
    "        # h contains the node representations computed from the GNN defined\n",
    "        # in the node classification section (Section 5.1)\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h']\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Given the node representation computation model and an edge predictor model, we can easily write a full-graph training loop where we compute the prediction on all edges.\n",
    "\n",
    "The following example takes `SAGE` in the previous section as the node representation computation model and `DotPredictor` as an edge predictor model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.sage = SAGE(in_features, hidden_features, out_features)\n",
    "        self.pred = DotProductPredictor()\n",
    "    def forward(self, g, x):\n",
    "        h = self.sage(g, x)\n",
    "        return self.pred(g,h)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we also assume that the training/validation/test edge sets are identified by boolean masks on edges. This example does not include early stopping or model saving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = edge_pred_graph.ndata['feature']\n",
    "edge_label = edge_pred_graph.edata['label']\n",
    "train_mask = edge_pred_graph.edata['train_mask']\n",
    "model = Model(10,20,5)\n",
    "opt = torch.optim.Adam(model.paramters())\n",
    "for epoch in range(10):\n",
    "    pred = model(edge_pred_graph, node_features)\n",
    "    loss = ((pred[train_mask] - edge_label[train_mask]) ** 2).mean()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous Graph\n",
    "\n",
    "Edge classification on heterogeneous graphs is not very different from that on homogeneous graphs. If you wish to perform edge classification on one edge type, you only need to compute the node representation for all node types, and predict on that edge type with the `apply_edges()` method.\n",
    "\n",
    "For example to make `DotProductPredictor` work on one edge type of a heterogeneous graph, you only need to specify the edge type in the `apply_edges()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDotProductPredictor(nn.Module):\n",
    "    def forward(self, graph, h, e_type):\n",
    "        # h contains the node rep for each edge type computed from the GNN\n",
    "        # for heterogeneous graphs defined in the node classification section 5.1 \n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h # assigns 'h' of all node types in one shot\n",
    "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'), etype=etype)\n",
    "            return graph.edges[etype].data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a similar fashion we write HeteroMLPPredictor\n",
    "class HeteroMLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "        \n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(torch.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "    \n",
    "    def forward(self, graph, h, etype):\n",
    "        # h contains the node reps for each edge type computed from \n",
    "        # the GNN in 5.1\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edgesm etype=etype)\n",
    "            return graph.edges[etype].data['score']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end-to-end model that predicts a score for each edge on a single edge type will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, rel_names):\n",
    "        super().__init__()\n",
    "        self.sage = RGCN(in_features, hidden_features, out_features, rel_names)\n",
    "        self.pred = HeteroDotProductPredictor\n",
    "    def forward(self, g, x, etype):\n",
    "        h = self.sage(g, x)\n",
    "        return self.pred(g, h, etype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model simply involves feeding the model a dictionary of node types and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(10, 20, 5, hetero_graph.etypes)\n",
    "user_feats = hetero_graph.nodes['user'].data['feature']\n",
    "item_feats = hetero_graph.nodes['item'].data['feature']\n",
    "label = hetero_graph.edges['click'].data['label']\n",
    "train_mask = hetero_graph.edges['click.data'].data['train_mask']\n",
    "node_features = {'user': user_feats, 'item': item_feats}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop looks almost the same as that in the homogeneous graph. For instance, if you wish to predict the edge labels on edge type `click`, then you can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(10):\n",
    "    pred = model(hetero_graph, node_features, 'click')\n",
    "    loss = ((pred[train_mask] - label[train_mask]) ** 2).mean()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Edge Type of an Existing Edge on a Heterogeneous Graph\n",
    "\n",
    "This problem arises when we want to predict which type an existing edge belongs to.\n",
    "\n",
    "This is a simplified version of rating prediction, which is common in recommeder tasks on graphs.\n",
    "\n",
    "We can use a heterogeneous graph convolution network to obtain the node representations. For instance, we can still use the RGCN defined ealier in chapter 5. \n",
    "\n",
    "To predict the type of an edge, we can simply repurpose `HeteroDotProductPredictor` above so that it takes another graph with only one edge type that 'merges' all the edge types to be predicted, and emits the score of each type for ever edge. \n",
    "\n",
    "In the example here, we will need a graph that has two node types `user` and `item`, and one single edge type that 'merges' all the edge types from `user` and `item`. That is it simplifies `click` and `dislike`. This can be conveniently created usint the following syntax:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_graph = hetero_graph['user', :, 'item']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which returns a heterogeneous graph with node type `user` and `item` as well as a single edge type combining all edge types in between, in effect `click` and `dislike`. \n",
    "\n",
    "Since the statement above also returns the original edge types as a feature named `dgl.ETYPE`, we can use that as labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_label = dec_graph.edata[dgl.ETYPE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the graph above as input to the edge type predictor module, you can write your predictor module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroMLPPredictor(nn.Module):\n",
    "    def __init__(self, in_dims, n_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_dims * 2, n_classes)\n",
    "    \n",
    "    def apply_edges(self, edges):\n",
    "        x = torch.cat([edges.src['h'], edge.dst['h']], 1)\n",
    "        y = self.W(x)\n",
    "        return {'score': y}\n",
    "    \n",
    "    def forward(self, graph, h):\n",
    "        # h contains the node representations for each edge type computed from\n",
    "        # THe GNN for heterogeneous graphs defined in the node classification section 5.1\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model combines the node representation module and the edge type predictor module is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, rel_names):\n",
    "        super().__init__()\n",
    "        self.sage = RGCN(in_features, hidden_features, out_features, rel_names)\n",
    "        self.pred = HeteroDotProductPredictor(out_features, len(rel_names))\n",
    "    def forward(self, g, x, dec_graph):\n",
    "        h = self.sage(g, x)\n",
    "        return self.pred(dec_graph, h)\n",
    "    \n",
    "# Associated training loop\n",
    "model =  Model(10,20,5,hetero_graph.etypes)\n",
    "user_feats = hetero_graph['user'].data['feature']\n",
    "item_feats = hetero_graph['item'].data['feature']\n",
    "node_features = {'user': user_feats, 'item': item_feats}\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(10):\n",
    "    logits = model(hetero_graph, node_features, dec_graph)\n",
    "    loss = F.cross_entropy(logits, edge_label)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL provides Graph Convolutional Matrix Completion as an example of rating prediction, which is formulated by predicting the type of an existing edge on a heterogeneous graph. The node representation module in the model implementation file is called `GCMCLayer`. The edge predictor module is called `BiDecoder`. Both of them are more complicated than the setting described here. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Link Prediction\n",
    "\n",
    "https://docs.dgl.ai/en/1.0.x/guide/training-link.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
