{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Graph Data Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dgl implements commonly used graph datasets in `dgl.data`. The pipeline for these datasets is defined in `dgl.data.DGLDataset`. DGL highly recommends processing graph data into a `dgl.data.Dataset` subclass, as the pipeline provides clean and simple solutions for loading, processing and saving graph data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 DGLDataset Class\n",
    "\n",
    "`DGLDataset` is the base class for processing, loading and saving graph datasets defined in `dgl.data`. `DGLDataset` implements the basic pipeline for processing graph data. The following flowchart shows how the pipeline works. \n",
    "\n",
    "![](https://data.dgl.ai/asset/image/userguide_data_flow.png)\n",
    "\n",
    "To process a graph dataset located in a remote server or local disk, once can define a class say `MyDataset`, inheriting from `dgl.data.DGLDataset`, The tempplate of `MyDataset` is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "\n",
    "class MyDataset(DGLDataset):\n",
    "    \"\"\" Template for customizing graph datasets in DGL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL to download the raw dataset\n",
    "    raw_dir : str\n",
    "        Specifying the directory that will store the\n",
    "        downloaded data or the directory that\n",
    "        already stores the input data.\n",
    "        Default: ~/.dgl/\n",
    "    save_dir : str\n",
    "        Directory to save the processed dataset.\n",
    "        Default: the value of `raw_dir`\n",
    "    force_reload : bool\n",
    "        Whether to reload the dataset. Default: False\n",
    "    verbose : bool\n",
    "        Whether to print out progress information\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 url=None,\n",
    "                 raw_dir=None,\n",
    "                 save_dir=None,\n",
    "                 force_reload=False,\n",
    "                 verbose=False):\n",
    "        super(MyDataset, self).__init__(name='dataset_name',\n",
    "                                        url=url,\n",
    "                                        raw_dir=raw_dir,\n",
    "                                        save_dir=save_dir,\n",
    "                                        force_reload=force_reload,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "    def download(self):\n",
    "        # download raw data to local disk\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # process raw data to graphs, labels, splitting masks\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get one example by index\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of data examples\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        # save processed data to directory `self.save_path`\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        # load processed data from directory `self.save_path`\n",
    "        pass\n",
    "\n",
    "    def has_cache(self):\n",
    "        # check whether there are processed data in `self.save_path`\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DGLDataset` class has abstract methods `process()`, `__getitem__(idx)` and `__len__()` that must be implemented in the subclass. DGL also recommends implementing saving and loading as well, since they can save significant time for processing large datasets and there are several APIs making it easy. \n",
    "\n",
    "`DGLDataset's` purpose is to provide a standard and convenient way to load graph data. One can store graphs, features, labels, masks and basic information about the dataset, such as number of classes, number of labels, etc. Operations such as sampling, partition or feature normalization are done outside of the `DGLDataset` subclass. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Download Raw Data\n",
    "\n",
    "If a dataset is already in local disk, make sure it's in a directory `raw_dir`. If we want to run the code anywhere without bothering to download and move data to the right directory, we can do so automatically by implementing function `download()`. \n",
    "\n",
    "If the dataset is a zip file, make `MyDataset`inherit from `dgl.data.DGLBuiltinDataset` class, which handles the zip file extraction for us. Otherwise we need to implement `download()` like in the `QM7BDataset`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dgl.data.utils import download\n",
    "\n",
    "def download(self):\n",
    "    # path to store the file\n",
    "    file_path = os.path.join(self.raw_dir, self.name + '.mat')\n",
    "    # download file\n",
    "    download(self.url, path=file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code downloads the a .mat file to the directory `self.raw_dir`. If the file is a .gz, .tar, .tar.gz, or .tgz file, we use `extract_archive()` to complete the extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data.utils import download, check_sha1\n",
    "\n",
    "def download(self):\n",
    "    # path to store the file\n",
    "    # make sure to use the same suffix as the original file name's\n",
    "    gz_file_path = os.path.join(self.raw_dir, self.name + '.csv.gz')\n",
    "    # download file\n",
    "    download(self.url, path=gz_file_path)\n",
    "    # check SHA-1\n",
    "    if not check_sha1(gz_file_path, self._sha1_str):\n",
    "        raise UserWarning('File {} is downloaded but the content hash does not match.'\n",
    "                          'The repo may be outdated or download may be incomplete. '\n",
    "                          'Otherwise you can create an issue for it.'.format(self.name + '.csv.gz'))\n",
    "    # extract file to directory `self.name` under `self.raw_dir`\n",
    "    self._extract_gz(gz_file_path, self.raw_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will extract the file into directory `self.name` under `self.raw_dir`. If the class inherits from `dgl.data.DGLBuiltinDataset` to handle zip file, it will extract the file into directory `self.name` as well. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Process Data\n",
    "\n",
    "We can implement the data processing code in the function `process()`, and it assumes that the raw data is located in `self.raw_dir` already. There are typically 3 types of tasks in machine learning on graphs: graph classification, node classification, and link prediction. \n",
    "\n",
    "### Processing Graph Classification Datasets\n",
    "\n",
    "Graph classifiction datasets are almost the same as most datasets in typical machine learning tasks, where mini-batch training is used. So one can process the raw data to a list of `dgl.DGLGraph` objects and a list of label tensors. In addition, if the raw data has been split into several files, one can add a parameter `split` to load a specific part of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "\n",
    "class QM7bDataset(DGLDataset):\n",
    "    _url = 'http://deepchem.io.s3-website-us-west-1.amazonaws.com/' \\\n",
    "           'datasets/qm7b.mat'\n",
    "    _sha1_str = '4102c744bb9d6fd7b40ac67a300e49cd87e28392'\n",
    "\n",
    "    def __init__(self, raw_dir=None, force_reload=False, verbose=False):\n",
    "        super(QM7bDataset, self).__init__(name='qm7b',\n",
    "                                          url=self._url,\n",
    "                                          raw_dir=raw_dir,\n",
    "                                          force_reload=force_reload,\n",
    "                                          verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        mat_path = self.raw_path + '.mat'\n",
    "        # process data to a list of graphs and a list of labels\n",
    "        self.graphs, self.label = self._load_graph(mat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get graph and label by index\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Item index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (dgl.DGLGraph, Tensor)\n",
    "        \"\"\"\n",
    "        return self.graphs[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of graphs in the dataset\"\"\"\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `process()`, the raw data is processed to a list of graphs and a list of labels. We must implement `__getitem__(idx)` and `__len__()` for iteration. DGL recommends making `__getitem__(idx)` return a tuple `(graph, label)`. \n",
    "\n",
    "We may also add properties to the class to indicate some useful information of the dataset. In `QM7bDataset`, we can add a property `num_tasks` to indicate the total number of prediction tasks in this multi-task dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property \n",
    "def num_tasks(self):\n",
    "    \"\"\"Number of labels for each graph\n",
    "    \"\"\"\n",
    "    return 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "# load data\n",
    "dataset = QM7bDataset()\n",
    "num_tasks = dataset.num_tasks\n",
    "\n",
    "# create dataloaders\n",
    "dataloader = GraphDataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# training\n",
    "for epoch in range(100):\n",
    "    for g, labels in dataloader:\n",
    "        # your training code here\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph classification datasets built into DGL are:\n",
    "- gindataset\n",
    "- minigcdataset\n",
    "- qm7bdata\n",
    "- tudata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Node Classification Datasets\n",
    "Node classification is usually performed on a single graph. Where splits of the dataset are done on the nodes of the graph. DGL suggests using node masks to specify splits. \n",
    "\n",
    "DGL recommends re-arrange the nodes and edges so that the nodes and edges so that nodes near to each other have IDs in a close range. The procedure could improve the locality to access a node's neighbours, which may benefit follow-up computation and analysis conducted on the graph. DGL provides an API called `dgl.reorder_graph()` for this purpose. We can see this in the `process()` part in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLBuiltinDataset\n",
    "from dgl.data.utils import _get_dgl_url\n",
    "\n",
    "class CitationGraphDataset(DGLBuiltinDataset):\n",
    "    _urls = {\n",
    "        'cora_v2' : 'dataset/cora_v2.zip',\n",
    "        'citeseer' : 'dataset/citeseer.zip',\n",
    "        'pubmed' : 'dataset/pubmed.zip',\n",
    "    }\n",
    "\n",
    "    def __init__(self, name, raw_dir=None, force_reload=False, verbose=True):\n",
    "        assert name.lower() in ['cora', 'citeseer', 'pubmed']\n",
    "        if name.lower() == 'cora':\n",
    "            name = 'cora_v2'\n",
    "        url = _get_dgl_url(self._urls[name])\n",
    "        super(CitationGraphDataset, self).__init__(name,\n",
    "                                                   url=url,\n",
    "                                                   raw_dir=raw_dir,\n",
    "                                                   force_reload=force_reload,\n",
    "                                                   verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        # Skip some processing code\n",
    "        # === data processing skipped ===\n",
    "\n",
    "        # build graph\n",
    "        g = dgl.graph(graph)\n",
    "        # splitting masks\n",
    "        g.ndata['train_mask'] = train_mask\n",
    "        g.ndata['val_mask'] = val_mask\n",
    "        g.ndata['test_mask'] = test_mask\n",
    "        # node labels\n",
    "        g.ndata['label'] = torch.tensor(labels)\n",
    "        # node features\n",
    "        g.ndata['feat'] = torch.tensor(_preprocess_features(features),\n",
    "                                       dtype=F.data_type_dict['float32'])\n",
    "        self._num_tasks = onehot_labels.shape[1]\n",
    "        self._labels = labels\n",
    "        # reorder graph to obtain better locality.\n",
    "        self._g = dgl.reorder_graph(g)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        return self._g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the data \n",
    "dataset = CiteseerGraphDataset(raw_dir='')\n",
    "graph = dataset[0]\n",
    "\n",
    "# Get split masks\n",
    "train_mask = graph.ndata['train_mask']\n",
    "val_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "\n",
    "# GEt node features \n",
    "feats = graph.ndata['feat']\n",
    "\n",
    "# get labels \n",
    "labels = graph.ndata['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples of node classification datasets, please refer to DGL’s builtin datasets:\n",
    "\n",
    "- citationdata\n",
    "- corafulldata\n",
    "- amazoncobuydata\n",
    "- coauthordata\n",
    "- karateclubdata\n",
    "- ppidata\n",
    "- redditdata\n",
    "- sbmdata\n",
    "- sstdata\n",
    "- rdfdata\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing dataset for Link Prediction datasets\n",
    "\n",
    "The processing of link prediction datasets is similar to that for node classification's. Usually performed on one graph. \n",
    "\n",
    "This section uses the builtin dataset `KnowledgeGraphDataset` as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for creating Link Prediction datasets\n",
    "class KnowledgeGraphDataset(DGLBuiltinDataset):\n",
    "    def __init__(self, name, reverse=True, raw_dir=None, force_reload=False, verbose=True):\n",
    "        self._name = name\n",
    "        self.reverse = reverse\n",
    "        url = _get_dgl_url('dataset/') + '{}.tgz'.format(name)\n",
    "        super(KnowledgeGraphDataset, self).__init__(name,\n",
    "                                                    url=url,\n",
    "                                                    raw_dir=raw_dir,\n",
    "                                                    force_reload=force_reload,\n",
    "                                                    verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        # Skip some processing code\n",
    "        # === data processing skipped ===\n",
    "\n",
    "        # splitting mask\n",
    "        g.edata['train_mask'] = train_mask\n",
    "        g.edata['val_mask'] = val_mask\n",
    "        g.edata['test_mask'] = test_mask\n",
    "        # edge type\n",
    "        g.edata['etype'] = etype\n",
    "        # node type\n",
    "        g.ndata['ntype'] = ntype\n",
    "        self._g = g\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        return self._g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code differs from the node classification task since it adds a splitting mask into the `edata` field of the graph.\n",
    "\n",
    "A commonly used subclass of the `KnowledgeGraphDataset` is `dgl.data.FB15k237Dataset` which can be seen below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import FB15k237Dataset\n",
    "\n",
    "# load data\n",
    "dataset = FB15k237Dataset()\n",
    "graph = dataset[0]\n",
    "\n",
    "# get training mask\n",
    "train_mask = graph.edata['train_mask']\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "src, dst = graph.edges(train_idx)\n",
    "# get edge types in training set\n",
    "rel = graph.edata['etype'][train_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Save and Load Data\n",
    "\n",
    "DGL recommends implementing saving and loading function to cache the processed data in local disk. DGL provides 4 helper functions to make this easier:\n",
    "\n",
    "- `dgl.save_graphs()` and `dgl.load_graphs()`: Save/load DGLGraph objects and labels to/from local disk. \n",
    "- `dgl.data.utils.save_info()` and `dgl.data.utils.load_info()`: save/load useful information of the dataset as a python dict object to/from disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dgl import save_graphs, load_graphs\n",
    "from dgl.data.utils import makedirs, save_info, load_info\n",
    "\n",
    "def save(self):\n",
    "    # save graphs and labels\n",
    "    graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "    save_graphs(graph_path, self.graphs, {'labels': self.labels})\n",
    "    # save other information in python dict\n",
    "    info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "    save_info(info_path, {'num_classes': self.num_classes})\n",
    "\n",
    "def load(self):\n",
    "    # load processed data from directory `self.save_path`\n",
    "    graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "    self.graphs, label_dict = load_graphs(graph_path)\n",
    "    self.labels = label_dict['labels']\n",
    "    info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "    self.num_classes = load_info(info_path)['num_classes']\n",
    "\n",
    "def has_cache(self):\n",
    "    # check whether there are processed data in `self.save_path`\n",
    "    graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "    info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "    return os.path.exists(graph_path) and os.path.exists(info_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there are cases not suitable to save processed data. For example, in the builtin dataset `GDELTDataset`, the processed data is large, so it's more efficient to process each data example in `__getitem__(idx)`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Loading OGB datasets using `ogb` package\n",
    "\n",
    "The `ogb` package contains APIS for downloading and processing OGB datasets into `dgl.data.DGLGraph` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the ogb package\n",
    "!pip install ogb\n",
    "\n",
    "# A code snippet for graph property prediction\n",
    "# Load Graph Property Prediction datasets in OGB\n",
    "import dgl\n",
    "import torch\n",
    "from ogb.graphproppred import DglGraphPropPredDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    # batch is a list of tuple (graph, label)\n",
    "    graphs = [e[0] for e in batch]\n",
    "    g = dgl.batch(graphs)\n",
    "    labels = [e[1] for e in batch]\n",
    "    labels = torch.stack(labels, 0)\n",
    "    return g, labels\n",
    "\n",
    "# load dataset\n",
    "dataset = DglGraphPropPredDataset(name='ogbg-molhiv')\n",
    "split_idx = dataset.get_idx_split()\n",
    "# dataloader\n",
    "train_loader = GraphDataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, collate_fn=_collate_fn)\n",
    "valid_loader = GraphDataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, collate_fn=_collate_fn)\n",
    "test_loader = GraphDataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, collate_fn=_collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading  Node Property prediction datasets is similar, but there is only one graph so we do not need to do the collation step. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Node Property Prediction datasets in OGB\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name='ogbn-proteins')\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "# there is only one graph in Node Property Prediction datasets\n",
    "g, labels = dataset[0]\n",
    "# get split labels\n",
    "train_label = dataset.labels[split_idx['train']]\n",
    "valid_label = dataset.labels[split_idx['valid']]\n",
    "test_label = dataset.labels[split_idx['test']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link prediction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Link Property Prediction datasets in OGB\n",
    "from ogb.linkproppred import DglLinkPropPredDataset\n",
    "\n",
    "dataset = DglLinkPropPredDataset(name='ogbl-ppa')\n",
    "split_edge = dataset.get_edge_split()\n",
    "\n",
    "graph = dataset[0]\n",
    "print(split_edge['train'].keys())\n",
    "print(split_edge['valid'].keys())\n",
    "print(split_edge['test'].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Loading Data From CSV Files\n",
    "\n",
    "Comma Separated Value (CSV) format can be read into DGL with the `CSVDataset` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl \n",
    "ds = dgl.data.CSVDataset('path/to/dataset/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned `ds` object is a standard `DGLDataset`. As such we have access to methods such as `__getitem__(idx)` as well as node/edge features using `ndata` and `edata` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demonstration of how to use the loaded dataset. The feature names\n",
    "# may vary depending on the CSV contents.\n",
    "g = ds[0] # get the graph\n",
    "label = g.ndata['label']\n",
    "feat = g.ndata['feat']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder Structure\n",
    "\n",
    "```\n",
    "/path/to/dataset/\n",
    "|-- meta.yaml     # metadata of the dataset\n",
    "|-- edges_0.csv   # edge data including src_id, dst_id, feature, label and so on\n",
    "|-- ...           # you can have as many CSVs for edge data as you want\n",
    "|-- nodes_0.csv   # node data including node_id, feature, label and so on\n",
    "|-- ...           # you can have as many CSVs for node data as you want\n",
    "|-- graphs.csv    # graph-level features\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node/edge/graph-level data are stored in CSV files. `meta.yml` is a metadata file specifying where to read nodes/edges/graphs data and how to parse them to construct the dataset object. A minimal data folder contains one `meta.yml` and two CSVs, one for node data and one for edge data. This occurs when the dataset contains only a single graph with no graph-level data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset of a single feature-less graph. \n",
    "\n",
    "When the dataset contains only one graph with no node or edge features, there need be only 3 files in the data folder: `meta.yml`, one CSV for node IDs and one CSV for edges. \n",
    "\n",
    "```\n",
    "./mini_featureless_dataset/\n",
    "|-- meta.yml\n",
    "|-- nodes.csv\n",
    "|-- edges.csv\n",
    "```\n",
    "\n",
    "`meta.yml` contains the following information: \n",
    "\n",
    "**dataset_name**: mini_featureless_dataset\n",
    "\n",
    "**edge_data**:\n",
    "- **file_name:** edges.csv\n",
    "\n",
    "**node_data**:\n",
    "\n",
    "- **file_name:** nodes.csv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nodes.csv` lists the node IDs under the `node_id` field:\n",
    "\n",
    "node_id  \n",
    "0  \n",
    "1  \n",
    "2  \n",
    "3  \n",
    "4  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`edges.csv` lists all the edges in the two columns (`src_id` and `dst_id`) specifying the source and destination node ID of each edge:\n",
    "\n",
    "src_id,dst_id  \n",
    "4,4  \n",
    "4,1  \n",
    "3,0  \n",
    "4,1  \n",
    "4,0  \n",
    "1,2  \n",
    "1,3  \n",
    "3,3  \n",
    "1,1  \n",
    "4,1  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset has one graph without any features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "dataset = dgl.data.CSVDataset('./mini_featureless_dataset')\n",
    "g = dataset[0]\n",
    "print(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Non-integer node IDs are allowed. When constructing the graph, `CSVDataset` will map each raw ID to and integer ID starting from zero. If the node IDs are already distinct integers from 0 to `num_nodes-1`, no mapping is applied.\n",
    "\n",
    "Node: Edges are always directed. To have both directions, add reversed edges in the edge CSV file or use `AddReverse` to transform the loaded graph. \n",
    "\n",
    "A graph without any feature is often of less interest. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset of a single graph with features and labels\n",
    "\n",
    "When the dataset contains a single graph with node or edge features and labels, there still need be only thre files in the data folder: `meta.yml`, one CSV for node IDs and one CSV for edges:\n",
    "\n",
    "```\n",
    "./mini_feature_dataset/\n",
    "|-- meta.yml  \n",
    "|-- nodes.csv  \n",
    "|-- edges.csv\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`meta.yml`:\n",
    "**dataset_name:** mini_feature_dataset  \n",
    "**edge_data:** \n",
    "\n",
    "- **file_name:** edges.csv\n",
    "\n",
    "**node_data:**\n",
    "\n",
    "- **file_name:** nodes.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`edges.csv` with five synthetic edge data (`label`, `train_mask`, `val_mask`, `test_mask`, `feat`):\n",
    "\n",
    "```\n",
    "src_id,dst_id,label,train_mask,val_mask,test_mask,feat\n",
    "4,0,2,False,True,True,\"0.5477868606453535, 0.4470617033458436, 0.936706701616337\"\n",
    "4,0,0,False,False,True,\"0.9794634290792008, 0.23682038840665198, 0.049629338970987646\"\n",
    "0,3,1,True,True,True,\"0.8586722047523594, 0.5746912787380253, 0.6462162561249654\"\n",
    "0,1,2,True,False,False,\"0.2730008213674695, 0.5937484188166621, 0.765544096939567\"\n",
    "0,2,1,True,True,True,\"0.45441619816038514, 0.1681403185591509, 0.9952376085297715\"\n",
    "0,0,0,False,False,False,\"0.4197669213305396, 0.849983324532477, 0.16974127573016262\"\n",
    "2,2,1,False,True,True,\"0.5495035052928215, 0.21394654203489705, 0.7174910641836348\"\n",
    "1,0,2,False,True,False,\"0.008790817766266334, 0.4216530595907526, 0.529195480661293\"\n",
    "3,0,0,True,True,True,\"0.6598715708878852, 0.1932390907048961, 0.9774471538377553\"\n",
    "4,0,1,False,False,False,\"0.16846068931179736, 0.41516080644186737, 0.002158116134429955\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nodes.csv` with five synthetic node data (`label`, `train_mask`, `val_mask`, `test_mask`, `feat`):\n",
    "\n",
    "```\n",
    "node_id,label,train_mask,val_mask,test_mask,feat\n",
    "0,1,False,True,True,\"0.07816474278491703, 0.9137336384979067, 0.4654086994009452\"\n",
    "1,1,True,True,True,\"0.05354099924658973, 0.8753101998792645, 0.33929432608774135\"\n",
    "2,1,True,False,True,\"0.33234211884156384, 0.9370522452510665, 0.6694943496824788\"\n",
    "3,0,False,True,False,\"0.9784264442230887, 0.22131880861864428, 0.3161154827254189\"\n",
    "4,1,True,True,False,\"0.23142237259162102, 0.8715767748481147, 0.19117861103555467\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After being loaded, the dataset has one graph. Node/edge features are stored in `ndata` and `edata` with the same column names. The example demonstrates how to specify a vector-shaped feature using comma-separated list enclosed by double quotes `\"...\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl \n",
    "dataset = dgl.data.CSVDataset('./mini_feature_dataset')\n",
    "g = dataset[0]\n",
    "print(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default, `CSVDataset` assumes all feature data to be numerical values (e.g. int, float, bool, or list) and missing values are not allowed. Users could provide custom data parser for these cases. \n",
    "\n",
    "### Dataset of a single heterogeneous graph\n",
    "\n",
    "One can specify multiple node and edge CSV files (each for one type) to represent a heterogeneous graph. Here is an example with two node types and two edge types. \n",
    "\n",
    "```\n",
    "./mini_hetero_dataset/\n",
    "|-- meta.yaml\n",
    "|-- nodes_0.csv\n",
    "|-- nodes_1.csv\n",
    "|-- edges_0.csv\n",
    "|-- edges_1.csv\n",
    "```\n",
    "\n",
    "The `meta.yml` specifies the node type name (using `ntype`) and edge type name (using `etype`) of each csv file. The edge type name is a string triplet containing the source node type name, relation name and the destination node type name. \n",
    "\n",
    "\n",
    "```\n",
    "dataset_name: mini_hetero_dataset\n",
    "edge_data:\n",
    "- file_name: edges_0.csv\n",
    "  etype: [user, follow, user]\n",
    "- file_name: edges_1.csv\n",
    "  etype: [user, like, item]\n",
    "node_data:\n",
    "- file_name: nodes_0.csv\n",
    "  ntype: user\n",
    "- file_name: nodes_1.csv\n",
    "  ntype: item\n",
    "  ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node and edge CSV files follow the same format as in heterogeneous graphs. Here are some synthetic data for demonstration purposes:\n",
    "\n",
    "`edges_0.csv` and `edges_1.csv`:\n",
    "\n",
    "```\n",
    "src_id,dst_id,label,feat\n",
    "4,4,1,\"0.736833152378035,0.10522806046048205,0.9418796835016118\"\n",
    "3,4,2,\"0.5749339182767451,0.20181320245665535,0.490938012147181\"\n",
    "1,4,2,\"0.7697294432580938,0.49397782380750765,0.10864079337442234\"\n",
    "0,4,0,\"0.1364240150959487,0.1393107840629273,0.7901988878812207\"\n",
    "2,3,1,\"0.42988138237505735,0.18389137408509248,0.18431292077750894\"\n",
    "0,4,2,\"0.8613368738351794,0.67985810014162,0.6580438064356824\"\n",
    "2,4,1,\"0.6594951663841697,0.26499036865016423,0.7891429392727503\"\n",
    "4,1,0,\"0.36649684241348557,0.9511783938523962,0.8494919263589972\"\n",
    "1,1,2,\"0.698592283371875,0.038622249776255946,0.5563827995742111\"\n",
    "0,4,1,\"0.5227112950269823,0.3148264185956532,0.47562693094002173\"\n",
    "```\n",
    "\n",
    "`nodes_0.csv` and `nodes_1.csv`:\n",
    "\n",
    "```\n",
    "node_id,label,feat\n",
    "0,2,\"0.5400687466285844,0.7588441197954202,0.4268254673041745\"\n",
    "1,1,\"0.08680051341900807,0.11446843700743892,0.7196969604886617\"\n",
    "2,2,\"0.8964389655603473,0.23368113896545695,0.8813472954005022\"\n",
    "3,1,\"0.5454703921677284,0.7819383771535038,0.3027939452162367\"\n",
    "4,1,\"0.5365210052235699,0.8975240205792763,0.7613943085507672\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the graph we have one heterogeneous graph with features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "dataset = dgl.data.CSVDataset('./mini_hetero_dataset')\n",
    "g = dataset[0]\n",
    "print(g)\n",
    "\n",
    "g.nodes['user'].data\n",
    "\n",
    "g.edges['like'].data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset of multiple graphs\n",
    "\n",
    "When there are multiple graphs, we can include an additional CSV file for storing graph-level features. Ex:\n",
    "\n",
    "```\n",
    "./mini_multi_dataset/\n",
    "|-- meta.yaml\n",
    "|-- nodes.csv\n",
    "|-- edges.csv\n",
    "|-- graphs.csv\n",
    "```\n",
    "\n",
    "Accordingly the `meta.yml` should include an extra `graph_data` key to tell which CSV file to load graph-level features from:\n",
    "\n",
    "```\n",
    "dataset_name: mini_multi_dataset\n",
    "edge_data:\n",
    "- file_name: edges.csv\n",
    "node_data:\n",
    "- file_name: nodes.csv\n",
    "graph_data:\n",
    "  file_name: graphs.csv\n",
    "```\n",
    "\n",
    "To distinguish nodes and edges of different graphs, the `node.csv` and `edge.csv` must contain an extra column, `graph_id`:\n",
    "\n",
    "`edges.csv`\n",
    "\n",
    "```\n",
    "graph_id,src_id,dst_id,feat\n",
    "0,0,4,\"0.39534097273254654,0.9422093637539785,0.634899790318452\"\n",
    "0,3,0,\"0.04486384200747007,0.6453746567017163,0.8757520744192612\"\n",
    "0,3,2,\"0.9397636966928355,0.6526403892728874,0.8643238446466464\"\n",
    "0,1,1,\"0.40559906615287566,0.9848072295736628,0.493888090726854\"\n",
    "0,4,1,\"0.253458867276219,0.9168191778828504,0.47224962583565544\"\n",
    "0,0,1,\"0.3219496197945605,0.3439899477636117,0.7051530741717352\"\n",
    "0,2,1,\"0.692873149428549,0.4770019763881086,0.21937428942781778\"\n",
    "0,4,0,\"0.620118223673067,0.08691420300562658,0.86573472329756\"\n",
    "0,2,1,\"0.00743445923710373,0.5251800239734318,0.054016385555202384\"\n",
    "0,4,1,\"0.6776417760682221,0.7291568018841328,0.4523600060547709\"\n",
    "1,1,3,\"0.6375445528248924,0.04878384701995819,0.4081642382536248\"\n",
    "1,0,4,\"0.776002616178397,0.8851294998284638,0.7321742043493028\"\n",
    "1,1,0,\"0.0928555079874982,0.6156748364694707,0.6985674921582508\"\n",
    "1,0,2,\"0.31328748118329997,0.8326121496142408,0.04133991340612775\"\n",
    "1,1,0,\"0.36786902637778773,0.39161865931662243,0.9971749359397111\"\n",
    "1,1,1,\"0.4647410679872376,0.8478810655406659,0.6746269314422184\"\n",
    "1,0,2,\"0.8117650553546695,0.7893727601272978,0.41527155506593394\"\n",
    "1,1,3,\"0.40707309111756307,0.2796588354307046,0.34846782265758314\"\n",
    "1,1,0,\"0.18626464175355095,0.3523777809254057,0.7863421810531344\"\n",
    "1,3,0,\"0.28357022069634585,0.13774964202156292,0.5913335505943637\"\n",
    "```\n",
    "\n",
    "`nodes.csv`:\n",
    "\n",
    "```\n",
    "graph_id,node_id,feat\n",
    "0,0,\"0.5725330322207948,0.8451870383322376,0.44412796119211184\"\n",
    "0,1,\"0.6624186423087752,0.6118386331195641,0.7352138669985214\"\n",
    "0,2,\"0.7583372765843964,0.15218126307872892,0.6810484348765842\"\n",
    "0,3,\"0.14627522432017592,0.7457985352827006,0.1037097085190507\"\n",
    "0,4,\"0.49037522512771525,0.8778998699783784,0.0911194482288028\"\n",
    "1,0,\"0.11158102039672668,0.08543289788089736,0.6901745368284345\"\n",
    "1,1,\"0.28367647637469273,0.07502571020414439,0.01217200152200748\"\n",
    "1,2,\"0.2472495901894738,0.24285506608575758,0.6494437360242048\"\n",
    "1,3,\"0.5614197853127827,0.059172654879085296,0.4692371689047904\"\n",
    "1,4,\"0.17583413999295983,0.5191278830882644,0.8453123358491914\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `graphs.csv` contains a `graph_id` column and arbitrary number of feature columns. The example dataset here has two graphs, each with a `feat` and a `label` graph-level data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "graph_id,feat,label\n",
    "0,\"0.7426272601929126,0.5197462471155317,0.8149104951283953\",0\n",
    "1,\"0.534822233529295,0.2863627767733977,0.1154897249106891\",0\n",
    "\n",
    "After loading, the dataset has multiple homographs with features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "dataset = dgl.data.CSVDataset('./mini_multi_dataset')\n",
    "print(len(dataset))\n",
    "graph0, data0 = dataset[0]\n",
    "print(graph0)\n",
    "\n",
    "print(data0)\n",
    "\n",
    "print(graph1)\n",
    "\n",
    "print(data1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a single feature column in `graphs.csv`, data0 will directly be a tensor for the feature.\n",
    "\n",
    "### Custom Data Parser\n",
    "\n",
    "By default, `CSVDataset` assumes that all the stored node/edge/graph level data are numerical values. Users can provide custom `DataParser` to `CSVDataset` to handle more complex data type. A `DataParser` needs to implement the `__call__` method which takes in the `pandas.DataFrame` object created from CSV file and should return a dictionary of parsed feature data. The parsed feature data will be saved to the `ndata` and `edata` of the corresponding `DGLGraph` object, and thus must be tensors or numpy arrays. Below shows an example `DataParser` which converts string type labels to integer:\n",
    "\n",
    "Given a dataset as follows, \n",
    "\n",
    "```\n",
    "./customized_parser_dataset/\n",
    "|-- meta.yaml\n",
    "|-- nodes.csv\n",
    "|-- edges.csv\n",
    "```\n",
    "\n",
    "`edges.csv`:\n",
    "\n",
    "```\n",
    "src_id,dst_id,label\n",
    "4,0,positive\n",
    "4,0,negative\n",
    "0,3,positive\n",
    "0,1,positive\n",
    "0,2,negative\n",
    "0,0,positive\n",
    "2,2,negative\n",
    "1,0,positive\n",
    "3,0,negative\n",
    "4,0,positive\n",
    "```\n",
    "\n",
    "`nodes.csv`:\n",
    "\n",
    "```\n",
    "node_id,label\n",
    "0,positive\n",
    "1,negative\n",
    "2,positive\n",
    "3,negative\n",
    "4,positive\n",
    "```\n",
    "\n",
    "To parse the string type labels, one can define a `DataParser` class as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MyDataParser:\n",
    "    def __call__(self, df:pd.DataFrame) -> dict:\n",
    "        parser = {}\n",
    "        for header in df:\n",
    "            if 'Unnamed' in header:\n",
    "                print(\"Unamed\")\n",
    "                continue\n",
    "            dt = df[header].to_numpy().squeeze()\n",
    "            if header == 'label':\n",
    "                dt = np.array([1 if e == 'positive' else 0 for e in dt])\n",
    "            parsed[header] = dt\n",
    "        return parsed\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `CSVDataset` using the defined `DataParser`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "dataset = dgl.data.CSVDataset('./customized_parser_dataset', \n",
    "                              ndata_parser=MyDataParser(), \n",
    "                              edata_parser=MyDataParser())\n",
    "\n",
    "print(dataset[0].ndata['label'])\n",
    "\n",
    "print(dataset[0].edata['label'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To specify different `DataParser`s for different node/edge types, pass a dictionary to `ndata_parser` and `edata_parser`, where the key is type name (a single string for node type; a string for edge type) and the value is the `DataParsers` to use. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full YAML Specification\n",
    "\n",
    "`CSVDataset` allows more flexible control over the loading and parsing process. For example, one can change the ID column names via `meta.yml`. The example below lists all the supported keys.\n",
    "\n",
    "```\n",
    "version: 1.0.0\n",
    "dataset_name: some_complex_data\n",
    "separator: ','                   # CSV separator symbol. Default: ','\n",
    "edge_data:\n",
    "- file_name: edges_0.csv\n",
    "  etype: [user, follow, user]\n",
    "  src_id_field: src_id           # Column name for source node IDs. Default: src_id\n",
    "  dst_id_field: dst_id           # Column name for destination node IDs. Default: dst_id\n",
    "- file_name: edges_1.csv\n",
    "  etype: [user, like, item]\n",
    "  src_id_field: src_id\n",
    "  dst_id_field: dst_id\n",
    "node_data:\n",
    "- file_name: nodes_0.csv\n",
    "  ntype: user\n",
    "  node_id_field: node_id         # Column name for node IDs. Default: node_id\n",
    "- file_name: nodes_1.csv\n",
    "  ntype: item\n",
    "  node_id_field: node_id         # Column name for node IDs. Default: node_id\n",
    "graph_data:\n",
    "  file_name: graphs.csv\n",
    "  graph_id_field: graph_id       # Column name for graph IDs. Default: graph_id\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the top level only 6 keys are available:\n",
    "\n",
    "- `version`: Optional. String;. Specifies which version of the `meta.yaml` is used. More features may be added in the future.\n",
    "\n",
    "- `dataset_name`: Required. String. It specifies the dataset name.\n",
    "\n",
    "- `separator`: Optional. String. It specifies how to parse data in CSV files. Default: `,`. \n",
    "\n",
    "- `edge_data`: Required. List of `EdgeData`. Metadata for parsing edges CSV files. \n",
    "\n",
    "- `node_data`: Required. List of `NodeData`. Metadata for parsing node CSV files. \n",
    "\n",
    "- `graph_data: Optional. `GraphData`. Metadata for parsing the graph CSV file. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`EdgeData`**\n",
    "\n",
    "There are 4 keys:\n",
    "\n",
    "- `file_name`: required. String. The CSV file to load data from. \n",
    "- `etype`: Optional. List of Strings. Edge type name in string format: [source node, relation type, destination type].\n",
    "- `src_id_field`: Optional. String. Which column to read for source node IDs. Default: `src_id`.\n",
    "- `dst_id_field`: Optional. String. Which column to read for destination node IDs. Default: `dst_id`.\n",
    "\n",
    "**`NodeData`**\n",
    "\n",
    "There are 3 keys:\n",
    "\n",
    "- `file_name`: Required. String. The CSV file to load data from. \n",
    "- `ntype`: Optional. String. Node type name. \n",
    "- `node_id_field`: Optional. String. Which column to read for node IDs. Default `node_id`.\n",
    "\n",
    "**`GraphData`** \n",
    "\n",
    "There are 2 keys: \n",
    "\n",
    "- `file_name`: Required. String. The CSV file to load data from. \n",
    "- `graph_id_field`: Optional. String. Which column to read for graph IDs. Default: `graph_id`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
