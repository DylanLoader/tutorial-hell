{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Write Your Own GNN Module\n",
    "Source: https://docs.dgl.ai/tutorials/blitz/3_message_passing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Passing and GNNs\n",
    "\n",
    "Message passing framework:\n",
    "\n",
    "$m_{u \\rightarrow v}^{(l)} = M^{(l)}(h_v^{(l-1)}, h_u^{(l-1)}, e_{u \\rightarrow v}^{(l-1)})$\n",
    "\n",
    "$m_v^{(l)} = \\sum_{u \\in \\mathcal{N}(v)} m_{u \\to v}^{(l)}$\n",
    "\n",
    "$h_v^{(l)} = U^{l} ( h_v^{(l-1)}, m_v^{(l)} )$\n",
    "\n",
    "Where $M^{(l)}$ is the message function, $\\sum$ is the reduce function (not necessarily a summation) and $U^{(l)}$ is the update function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we create the $\\sum$ reduce function to be the GraphSAGE Convolution as follows:\n",
    "\n",
    "$h_{\\mathcal{N}(v)}^k \\leftarrow Average\\{h_{u}^{k-1}, \\forall u \\in \\mathcal{N}(v)\\}$\n",
    "\n",
    "$h_v^k \\leftarrow ReLU (W^k \\cdot CONCAT(h_v^{k-1}, h_{\\mathcal{N}(v)}^k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConv(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        # A linear submodule to transform the input and neighbor features to the output.\n",
    "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Gathers and averages the neighborhood features. Update all triggers this for all nodes and edges.\n",
    "            g.update_all(\n",
    "                # Copies the node feature under name 'h' as messages under name 'm' which is sent to neighbors.\n",
    "                message_func=fn.copy_u(\"h\", \"m\"),\n",
    "                # Averages all the received messages under name 'm' and stores the result under name 'h_N'.\n",
    "                reduce_func=fn.mean(\"m\", \"h_N\"),\n",
    "            )\n",
    "            h_N = g.ndata[\"h_N\"]\n",
    "            h_total = torch.cat([h, h_N], dim=1)\n",
    "            return self.linear(h_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats)\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /Users/dloader/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
      "Extracting file to /Users/dloader/.dgl/cora_v2_d697a464\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "g = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    all_logits = []\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(200):\n",
    "        # Forward pass\n",
    "        logits = model(g, features)\n",
    "        \n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "        \n",
    "        # Compute loss\n",
    "        # Only computes the loss for the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        \n",
    "        # Compute train/val/test accuracy\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "        \n",
    "        # Save the best validation accuracy and the corresponding test accuracy\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_logits.append(logits.detach())\n",
    "        \n",
    "        if e % 5 == 0:\n",
    "            print(f\"In epoch {e}, loss: {loss:.4f}, val acc: {val_acc:.4f} (Best:{best_val_acc:.4f}), test acc: {test_acc:.4f} (Best:{best_test_acc:.4f})\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dloader/miniforge3/envs/dgl-env/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.9474, val acc: 0.1620 (Best:0.1620), test acc: 0.1490 (Best:0.1490)\n",
      "In epoch 5, loss: 1.8673, val acc: 0.2280 (Best:0.2280), test acc: 0.2610 (Best:0.2610)\n",
      "In epoch 10, loss: 1.7141, val acc: 0.5480 (Best:0.5480), test acc: 0.5370 (Best:0.5370)\n",
      "In epoch 15, loss: 1.4813, val acc: 0.6180 (Best:0.6180), test acc: 0.6300 (Best:0.6300)\n",
      "In epoch 20, loss: 1.1809, val acc: 0.6560 (Best:0.6560), test acc: 0.6700 (Best:0.6700)\n",
      "In epoch 25, loss: 0.8510, val acc: 0.6920 (Best:0.6920), test acc: 0.7230 (Best:0.7230)\n",
      "In epoch 30, loss: 0.5500, val acc: 0.7360 (Best:0.7360), test acc: 0.7570 (Best:0.7570)\n",
      "In epoch 35, loss: 0.3255, val acc: 0.7580 (Best:0.7580), test acc: 0.7690 (Best:0.7690)\n",
      "In epoch 40, loss: 0.1850, val acc: 0.7580 (Best:0.7580), test acc: 0.7710 (Best:0.7690)\n",
      "In epoch 45, loss: 0.1061, val acc: 0.7640 (Best:0.7640), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 50, loss: 0.0639, val acc: 0.7640 (Best:0.7640), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 55, loss: 0.0412, val acc: 0.7680 (Best:0.7680), test acc: 0.7730 (Best:0.7740)\n",
      "In epoch 60, loss: 0.0286, val acc: 0.7680 (Best:0.7680), test acc: 0.7720 (Best:0.7740)\n",
      "In epoch 65, loss: 0.0213, val acc: 0.7660 (Best:0.7680), test acc: 0.7720 (Best:0.7740)\n",
      "In epoch 70, loss: 0.0167, val acc: 0.7680 (Best:0.7680), test acc: 0.7700 (Best:0.7740)\n",
      "In epoch 75, loss: 0.0138, val acc: 0.7680 (Best:0.7680), test acc: 0.7710 (Best:0.7740)\n",
      "In epoch 80, loss: 0.0117, val acc: 0.7680 (Best:0.7680), test acc: 0.7690 (Best:0.7740)\n",
      "In epoch 85, loss: 0.0103, val acc: 0.7680 (Best:0.7680), test acc: 0.7720 (Best:0.7740)\n",
      "In epoch 90, loss: 0.0091, val acc: 0.7680 (Best:0.7680), test acc: 0.7710 (Best:0.7740)\n",
      "In epoch 95, loss: 0.0083, val acc: 0.7680 (Best:0.7680), test acc: 0.7720 (Best:0.7740)\n",
      "In epoch 100, loss: 0.0075, val acc: 0.7680 (Best:0.7680), test acc: 0.7720 (Best:0.7740)\n",
      "In epoch 105, loss: 0.0069, val acc: 0.7660 (Best:0.7680), test acc: 0.7730 (Best:0.7740)\n",
      "In epoch 110, loss: 0.0064, val acc: 0.7680 (Best:0.7680), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 115, loss: 0.0060, val acc: 0.7680 (Best:0.7680), test acc: 0.7730 (Best:0.7740)\n",
      "In epoch 120, loss: 0.0056, val acc: 0.7700 (Best:0.7700), test acc: 0.7730 (Best:0.7740)\n",
      "In epoch 125, loss: 0.0052, val acc: 0.7700 (Best:0.7700), test acc: 0.7750 (Best:0.7740)\n",
      "In epoch 130, loss: 0.0049, val acc: 0.7680 (Best:0.7700), test acc: 0.7750 (Best:0.7740)\n",
      "In epoch 135, loss: 0.0046, val acc: 0.7680 (Best:0.7700), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 140, loss: 0.0044, val acc: 0.7680 (Best:0.7700), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 145, loss: 0.0041, val acc: 0.7680 (Best:0.7700), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 150, loss: 0.0039, val acc: 0.7680 (Best:0.7700), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 155, loss: 0.0037, val acc: 0.7660 (Best:0.7700), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 160, loss: 0.0035, val acc: 0.7660 (Best:0.7700), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 165, loss: 0.0033, val acc: 0.7660 (Best:0.7700), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 170, loss: 0.0032, val acc: 0.7660 (Best:0.7700), test acc: 0.7740 (Best:0.7740)\n",
      "In epoch 175, loss: 0.0030, val acc: 0.7660 (Best:0.7700), test acc: 0.7730 (Best:0.7740)\n",
      "In epoch 180, loss: 0.0029, val acc: 0.7660 (Best:0.7700), test acc: 0.7720 (Best:0.7740)\n",
      "In epoch 185, loss: 0.0028, val acc: 0.7660 (Best:0.7700), test acc: 0.7720 (Best:0.7740)\n",
      "In epoch 190, loss: 0.0027, val acc: 0.7660 (Best:0.7700), test acc: 0.7720 (Best:0.7740)\n",
      "In epoch 195, loss: 0.0025, val acc: 0.7660 (Best:0.7700), test acc: 0.7720 (Best:0.7740)\n"
     ]
    }
   ],
   "source": [
    "model = Model(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSAGEConv(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(WeightedSAGEConv, self).__init__()\n",
    "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
    "        \n",
    "    def forward(self, g, h, w):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.edata['w'] = w\n",
    "            g.update_all(\n",
    "                message_func=fn.u_mul_e(\"h\", \"w\", \"m\"),\n",
    "                reduce_func=fn.mean(\"m\", \"h_N\"),\n",
    "            )\n",
    "            h_N = g.ndata[\"h_N\"]\n",
    "            h_total = torch.cat([h, h_N], dim=1)\n",
    "            return self.linear(h_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.9532, val acc: 0.1560 (Best:0.1560), test acc: 0.1440 (Best:0.1440)\n",
      "In epoch 5, loss: 1.8732, val acc: 0.4580 (Best:0.4580), test acc: 0.4490 (Best:0.4490)\n",
      "In epoch 10, loss: 1.7286, val acc: 0.3340 (Best:0.4620), test acc: 0.3460 (Best:0.4800)\n",
      "In epoch 15, loss: 1.5148, val acc: 0.4340 (Best:0.4620), test acc: 0.4330 (Best:0.4800)\n",
      "In epoch 20, loss: 1.2393, val acc: 0.5480 (Best:0.5480), test acc: 0.5360 (Best:0.5360)\n",
      "In epoch 25, loss: 0.9312, val acc: 0.6640 (Best:0.6640), test acc: 0.6540 (Best:0.6540)\n",
      "In epoch 30, loss: 0.6383, val acc: 0.7200 (Best:0.7200), test acc: 0.7180 (Best:0.7180)\n",
      "In epoch 35, loss: 0.4030, val acc: 0.7480 (Best:0.7480), test acc: 0.7330 (Best:0.7330)\n",
      "In epoch 40, loss: 0.2416, val acc: 0.7540 (Best:0.7560), test acc: 0.7450 (Best:0.7420)\n",
      "In epoch 45, loss: 0.1432, val acc: 0.7660 (Best:0.7680), test acc: 0.7460 (Best:0.7450)\n",
      "In epoch 50, loss: 0.0869, val acc: 0.7660 (Best:0.7680), test acc: 0.7490 (Best:0.7450)\n",
      "In epoch 55, loss: 0.0555, val acc: 0.7600 (Best:0.7680), test acc: 0.7480 (Best:0.7450)\n",
      "In epoch 60, loss: 0.0378, val acc: 0.7600 (Best:0.7680), test acc: 0.7510 (Best:0.7450)\n",
      "In epoch 65, loss: 0.0275, val acc: 0.7660 (Best:0.7680), test acc: 0.7520 (Best:0.7450)\n",
      "In epoch 70, loss: 0.0211, val acc: 0.7640 (Best:0.7680), test acc: 0.7550 (Best:0.7450)\n",
      "In epoch 75, loss: 0.0170, val acc: 0.7640 (Best:0.7680), test acc: 0.7540 (Best:0.7450)\n",
      "In epoch 80, loss: 0.0142, val acc: 0.7660 (Best:0.7680), test acc: 0.7550 (Best:0.7450)\n",
      "In epoch 85, loss: 0.0122, val acc: 0.7660 (Best:0.7680), test acc: 0.7560 (Best:0.7450)\n",
      "In epoch 90, loss: 0.0107, val acc: 0.7640 (Best:0.7680), test acc: 0.7580 (Best:0.7450)\n",
      "In epoch 95, loss: 0.0096, val acc: 0.7640 (Best:0.7680), test acc: 0.7570 (Best:0.7450)\n",
      "In epoch 100, loss: 0.0087, val acc: 0.7660 (Best:0.7680), test acc: 0.7600 (Best:0.7450)\n",
      "In epoch 105, loss: 0.0079, val acc: 0.7660 (Best:0.7680), test acc: 0.7580 (Best:0.7450)\n",
      "In epoch 110, loss: 0.0073, val acc: 0.7680 (Best:0.7680), test acc: 0.7580 (Best:0.7450)\n",
      "In epoch 115, loss: 0.0067, val acc: 0.7680 (Best:0.7680), test acc: 0.7590 (Best:0.7450)\n",
      "In epoch 120, loss: 0.0063, val acc: 0.7680 (Best:0.7680), test acc: 0.7580 (Best:0.7450)\n",
      "In epoch 125, loss: 0.0058, val acc: 0.7680 (Best:0.7680), test acc: 0.7580 (Best:0.7450)\n",
      "In epoch 130, loss: 0.0055, val acc: 0.7680 (Best:0.7680), test acc: 0.7580 (Best:0.7450)\n",
      "In epoch 135, loss: 0.0051, val acc: 0.7700 (Best:0.7700), test acc: 0.7580 (Best:0.7580)\n",
      "In epoch 140, loss: 0.0048, val acc: 0.7700 (Best:0.7700), test acc: 0.7570 (Best:0.7580)\n",
      "In epoch 145, loss: 0.0046, val acc: 0.7700 (Best:0.7700), test acc: 0.7570 (Best:0.7580)\n",
      "In epoch 150, loss: 0.0043, val acc: 0.7700 (Best:0.7700), test acc: 0.7570 (Best:0.7580)\n",
      "In epoch 155, loss: 0.0041, val acc: 0.7720 (Best:0.7720), test acc: 0.7570 (Best:0.7570)\n",
      "In epoch 160, loss: 0.0039, val acc: 0.7720 (Best:0.7720), test acc: 0.7570 (Best:0.7570)\n",
      "In epoch 165, loss: 0.0037, val acc: 0.7720 (Best:0.7720), test acc: 0.7580 (Best:0.7570)\n",
      "In epoch 170, loss: 0.0035, val acc: 0.7720 (Best:0.7720), test acc: 0.7580 (Best:0.7570)\n",
      "In epoch 175, loss: 0.0033, val acc: 0.7720 (Best:0.7720), test acc: 0.7580 (Best:0.7570)\n",
      "In epoch 180, loss: 0.0032, val acc: 0.7720 (Best:0.7720), test acc: 0.7570 (Best:0.7570)\n",
      "In epoch 185, loss: 0.0030, val acc: 0.7720 (Best:0.7720), test acc: 0.7580 (Best:0.7570)\n",
      "In epoch 190, loss: 0.0029, val acc: 0.7720 (Best:0.7720), test acc: 0.7580 (Best:0.7570)\n",
      "In epoch 195, loss: 0.0028, val acc: 0.7720 (Best:0.7720), test acc: 0.7580 (Best:0.7570)\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = WeightedSAGEConv(in_feats, h_feats)\n",
    "        self.conv2 = WeightedSAGEConv(h_feats, num_classes)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat, torch.ones(g.num_edges(),1).to(g.device))\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h, torch.ones(g.num_edges(),1).to(g.device))\n",
    "        return h\n",
    "\n",
    "model = Model(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeightedSageConvolution Implementation\n",
    "\n",
    "with the API under the dgl.function package we can use the built-in message and reduce functions to create new graph convolutions. The following SageConv class aggregates neighbor representations using a weighted average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSageConv(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(WeightedSAGEConv, self).__init__()\n",
    "        # A linear submodule that projects the input and neighbor feature to the output\n",
    "        self.linear = nn.Linear(in_feat*2, out_feat)\n",
    "        \n",
    "    def forward(self, g, h, w):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.edata['w'] = w\n",
    "            g.update_all(\n",
    "                message_func=fn.u_mul_e(\"h\", \"w\", \"m\"),\n",
    "                reduce_func=fn.mean(\"m\", \"h_N\"),\n",
    "                         )\n",
    "            h_N = g.ndata[\"h_N\"]\n",
    "            h_total = torch.cat([h, h_N], dim=1)\n",
    "            return self.linear(h_total)\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = WeightedSAGEConv(in_feats, h_feats)\n",
    "        self.conv2 = WeightedSAGEConv(h_feats, num_classes)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat, torch.ones(g.num_edges(), 1).to(g.device))\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h, torch.ones(g.num_edges(), 1).to(g.device))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.9478, val acc: 0.3160 (Best:0.3160), test acc: 0.3190 (Best:0.3190)\n",
      "In epoch 5, loss: 1.8653, val acc: 0.5620 (Best:0.5620), test acc: 0.5440 (Best:0.5440)\n",
      "In epoch 10, loss: 1.6988, val acc: 0.4780 (Best:0.5620), test acc: 0.4630 (Best:0.5440)\n",
      "In epoch 15, loss: 1.4456, val acc: 0.4620 (Best:0.5620), test acc: 0.4670 (Best:0.5440)\n",
      "In epoch 20, loss: 1.1262, val acc: 0.5520 (Best:0.5620), test acc: 0.5390 (Best:0.5440)\n",
      "In epoch 25, loss: 0.7866, val acc: 0.6440 (Best:0.6440), test acc: 0.6200 (Best:0.6200)\n",
      "In epoch 30, loss: 0.4896, val acc: 0.7140 (Best:0.7140), test acc: 0.6930 (Best:0.6930)\n",
      "In epoch 35, loss: 0.2785, val acc: 0.7260 (Best:0.7260), test acc: 0.7250 (Best:0.7250)\n",
      "In epoch 40, loss: 0.1522, val acc: 0.7340 (Best:0.7340), test acc: 0.7380 (Best:0.7380)\n",
      "In epoch 45, loss: 0.0846, val acc: 0.7380 (Best:0.7380), test acc: 0.7480 (Best:0.7480)\n",
      "In epoch 50, loss: 0.0499, val acc: 0.7340 (Best:0.7380), test acc: 0.7560 (Best:0.7480)\n",
      "In epoch 55, loss: 0.0320, val acc: 0.7340 (Best:0.7380), test acc: 0.7580 (Best:0.7480)\n",
      "In epoch 60, loss: 0.0223, val acc: 0.7440 (Best:0.7440), test acc: 0.7570 (Best:0.7570)\n",
      "In epoch 65, loss: 0.0167, val acc: 0.7440 (Best:0.7440), test acc: 0.7560 (Best:0.7570)\n",
      "In epoch 70, loss: 0.0133, val acc: 0.7460 (Best:0.7460), test acc: 0.7570 (Best:0.7560)\n",
      "In epoch 75, loss: 0.0111, val acc: 0.7460 (Best:0.7460), test acc: 0.7590 (Best:0.7560)\n",
      "In epoch 80, loss: 0.0096, val acc: 0.7460 (Best:0.7460), test acc: 0.7600 (Best:0.7560)\n",
      "In epoch 85, loss: 0.0084, val acc: 0.7480 (Best:0.7500), test acc: 0.7610 (Best:0.7610)\n",
      "In epoch 90, loss: 0.0076, val acc: 0.7500 (Best:0.7500), test acc: 0.7620 (Best:0.7610)\n",
      "In epoch 95, loss: 0.0069, val acc: 0.7500 (Best:0.7500), test acc: 0.7620 (Best:0.7610)\n",
      "In epoch 100, loss: 0.0063, val acc: 0.7500 (Best:0.7500), test acc: 0.7620 (Best:0.7610)\n",
      "In epoch 105, loss: 0.0059, val acc: 0.7500 (Best:0.7500), test acc: 0.7620 (Best:0.7610)\n",
      "In epoch 110, loss: 0.0054, val acc: 0.7500 (Best:0.7500), test acc: 0.7620 (Best:0.7610)\n",
      "In epoch 115, loss: 0.0051, val acc: 0.7520 (Best:0.7520), test acc: 0.7620 (Best:0.7620)\n",
      "In epoch 120, loss: 0.0048, val acc: 0.7520 (Best:0.7520), test acc: 0.7610 (Best:0.7620)\n",
      "In epoch 125, loss: 0.0045, val acc: 0.7520 (Best:0.7520), test acc: 0.7600 (Best:0.7620)\n",
      "In epoch 130, loss: 0.0042, val acc: 0.7520 (Best:0.7520), test acc: 0.7600 (Best:0.7620)\n",
      "In epoch 135, loss: 0.0040, val acc: 0.7520 (Best:0.7520), test acc: 0.7590 (Best:0.7620)\n",
      "In epoch 140, loss: 0.0038, val acc: 0.7500 (Best:0.7520), test acc: 0.7580 (Best:0.7620)\n",
      "In epoch 145, loss: 0.0036, val acc: 0.7500 (Best:0.7520), test acc: 0.7560 (Best:0.7620)\n",
      "In epoch 150, loss: 0.0034, val acc: 0.7500 (Best:0.7520), test acc: 0.7560 (Best:0.7620)\n",
      "In epoch 155, loss: 0.0032, val acc: 0.7480 (Best:0.7520), test acc: 0.7570 (Best:0.7620)\n",
      "In epoch 160, loss: 0.0031, val acc: 0.7460 (Best:0.7520), test acc: 0.7570 (Best:0.7620)\n",
      "In epoch 165, loss: 0.0029, val acc: 0.7460 (Best:0.7520), test acc: 0.7570 (Best:0.7620)\n",
      "In epoch 170, loss: 0.0028, val acc: 0.7460 (Best:0.7520), test acc: 0.7570 (Best:0.7620)\n",
      "In epoch 175, loss: 0.0027, val acc: 0.7460 (Best:0.7520), test acc: 0.7580 (Best:0.7620)\n",
      "In epoch 180, loss: 0.0025, val acc: 0.7460 (Best:0.7520), test acc: 0.7580 (Best:0.7620)\n",
      "In epoch 185, loss: 0.0024, val acc: 0.7460 (Best:0.7520), test acc: 0.7580 (Best:0.7620)\n",
      "In epoch 190, loss: 0.0023, val acc: 0.7460 (Best:0.7520), test acc: 0.7580 (Best:0.7620)\n",
      "In epoch 195, loss: 0.0022, val acc: 0.7440 (Best:0.7520), test acc: 0.7580 (Best:0.7620)\n"
     ]
    }
   ],
   "source": [
    "model = Model(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization by User-defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_mul_e_udf(edges):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        edges (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    return {\"m\": edges.src[\"h\"] * edges.data[\"w\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_udf(nodes):\n",
    "    \"\"\"The equivalent of dgl's built-in\n",
    "\n",
    "    Args:\n",
    "        nodes (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    return {\"h_N\": nodes.mailbox[\"m\"].mean(1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl-tutorial-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
